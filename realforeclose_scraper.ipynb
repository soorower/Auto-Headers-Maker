{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "realforeclose_scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVtfPopaYSPpNGaCzRALr9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soorower/Auto-Headers-Maker/blob/main/realforeclose_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RZJzelcthnDU"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output \n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first install gcsfuse\n",
        "%%capture\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt update\n",
        "!apt install gcsfuse\n",
        "\n",
        "\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)\n",
        "\n",
        "#you can install module with below format\n",
        "!pip install --target=$nb_path selenium"
      ],
      "metadata": {
        "id": "COkLg7bL5opY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "driver.maximize_window()\n",
        "\n",
        "\n",
        "\n",
        "headers = {\n",
        "'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n",
        "}\n",
        "\n",
        "listan = []\n",
        "datan  ={}\n",
        "def auction_scrape(date,types,left_side,right_side,county):\n",
        "    l_rows = left_side.findAll('div')\n",
        "    if l_rows[0].text == 'Auction Sold':\n",
        "        auction_status = 'Sold at '+ l_rows[1].text.strip()\n",
        "        sold_amount = l_rows[3].text.strip()\n",
        "        sold_to = l_rows[5].text.strip()\n",
        "    elif l_rows[0].text == 'Auction Starts':\n",
        "        auction_status = 'Starts at '+ l_rows[1].text.strip()\n",
        "        sold_amount = ''\n",
        "        sold_to = ''\n",
        "    else:\n",
        "        auction_status = l_rows[1].text.strip()\n",
        "        sold_amount = ''\n",
        "        sold_to = ''\n",
        "    #---------------------------------------------------------------------\n",
        "    try:\n",
        "        r_rows = right_side.find('table').find('tbody').findAll('tr')\n",
        "        row_count = 0\n",
        "        for row in r_rows:\n",
        "            if row.find('th').text == 'Auction Type:':\n",
        "                auction_type = row.find('td').text.strip()\n",
        "            elif row.find('th').text == 'Case #:':\n",
        "                case_hash = row.find('td').text.strip()\n",
        "            elif row.find('th').text == 'Final Judgment Amount:':\n",
        "                final_jud_amount = row.find('td').text.strip()\n",
        "            elif row.find('th').text == 'Parcel ID:':\n",
        "                parcel_id = row.find('td').text.strip()\n",
        "            elif row.find('th').text == 'Property Address:':\n",
        "                property_address = row.find('td').text.strip()\n",
        "                if r_rows[row_count+1].find('th').text == '':\n",
        "                    property_address = property_address +' '+ r_rows[row_count+1].find('td').text.strip()\n",
        "            elif row.find('th').text == 'Assessed Value:':\n",
        "                assessed_value = row.find('td').text.strip()\n",
        "            elif row.find('th').text == 'Plaintiff Max Bid:':\n",
        "                plaintiff_max_bid = row.find('td').text.strip()\n",
        "            row_count = row_count + 1\n",
        "    except:\n",
        "        r_rows = right_side.find('div',attrs = {'class':'ad_tab'}).findAll('div',attrs = {'class':'AD_DTA'})\n",
        "        auction_type = r_rows[0].text.strip()\n",
        "        case_hash = r_rows[1].text.strip()\n",
        "        final_jud_amount = r_rows[2].text.strip()\n",
        "        parcel_id = r_rows[3].text.strip()\n",
        "        try:\n",
        "            property_address = r_rows[4].text.strip()\n",
        "        except:\n",
        "            property_address = ''\n",
        "        try:\n",
        "            assessed_value = r_rows[5].text.strip()\n",
        "        except:\n",
        "            assessed_value = ''\n",
        "        try:\n",
        "            plaintiff_max_bid = r_rows[6].text.strip()\n",
        "        except:\n",
        "            plaintiff_max_bid = ''\n",
        "        \n",
        "        \n",
        "    try:\n",
        "        auction_type = auction_type\n",
        "    except:\n",
        "        auction_type = ''\n",
        "    try:\n",
        "        case_hash = case_hash\n",
        "    except:\n",
        "        case_hash = ''\n",
        "    try:\n",
        "        final_jud_amount = final_jud_amount\n",
        "    except:\n",
        "        final_jud_amount = ''\n",
        "    \n",
        "    try:\n",
        "        parcel_id = parcel_id\n",
        "    except:\n",
        "        parcel_id = ''\n",
        "    try:\n",
        "        property_address = property_address\n",
        "    except:\n",
        "        property_address = ''\n",
        "    try:\n",
        "        assessed_value = assessed_value\n",
        "    except:\n",
        "        assessed_value = ''\n",
        "    try:\n",
        "        plaintiff_max_bid = plaintiff_max_bid\n",
        "    except:\n",
        "        plaintiff_max_bid = ''\n",
        "\n",
        "    #----------geting owner name-------------------------------------------------------------------\n",
        "    new_parcel_id = parcel_id.replace('-','')\n",
        "    if county == 'duval':\n",
        "        owner_find_url = f'https://paopropertysearch.coj.net/Basic/Detail.aspx?RE={new_parcel_id}'\n",
        "        r = requests.get(owner_find_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "        try:\n",
        "            owner_name = soup.find('div',attrs= {'id':'ownerName'}).find('h2').text.strip()\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'miamidade':\n",
        "        owner_find_url = f'https://www.miamidade.gov/Apps/PA/PApublicServiceProxy/PaServicesProxy.ashx?Operation=GetPropertySearchByFolio&clientAppName=PropertySearch&folioNumber={new_parcel_id}'\n",
        "        r = requests.get(owner_find_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "\n",
        "        try:\n",
        "            own_n= str(soup).find('\"Name\":\"')\n",
        "            owner_name = str(soup)[own_n+8:own_n+50].split('\"')[0].strip()\n",
        "            if 'ionals'==owner_name:\n",
        "                owner_name = ''\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'broward':\n",
        "        owner_find_url = f'https://bcpa.net/RecInfo.asp?URL_Folio={new_parcel_id}'\n",
        "        r = requests.get(owner_find_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "\n",
        "        try:\n",
        "            owner_name = soup.findAll('span',attrs = {'class':'BodyCopyBold9'})[1].text.strip()\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'hillsborough':\n",
        "        owner_find_url = f'https://gis.hcpafl.org/CommonServices/property/search//ParcelData?pin={new_parcel_id}'\n",
        "        r = requests.get(owner_find_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "\n",
        "        try:\n",
        "            own_n= str(soup).find('\"owner\":\"')\n",
        "            owner_name = str(soup)[own_n+9:own_n+50].split('\"')[0].strip()\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'orange': \n",
        "        owner_find_url = f'https://ocpa-mainsite-afd-standard.azurefd.net/api/PRC/GetPRCGeneralInfo?pid={new_parcel_id}'\n",
        "        r = requests.get(owner_find_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "\n",
        "        try:\n",
        "            own_n= str(soup).find('\"ownerName\":\"')\n",
        "            owner_name = str(soup)[own_n+13:own_n+50].split('\"')[0].strip()\n",
        "            if 'ps://tools.ietf.org/html/rfc7231#sect' == owner_name:\n",
        "                owner_name = ''\n",
        "            if 'pid'== owner_name:\n",
        "                owner_name = ''\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'pinellas': \n",
        "        try:\n",
        "            new_parcel = parcel_id.split('-',3)\n",
        "            new_parcel_id = new_parcel[2] + new_parcel[1] +new_parcel[0] + new_parcel[-1].replace('-','')\n",
        "        except:\n",
        "            new_parcel_id = parcel_id.replace('-','')\n",
        "        owner_find_url = f'https://www.pcpao.org/general.php?strap={new_parcel_id}'\n",
        "        r = requests.get(owner_find_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "        try:\n",
        "            try:\n",
        "                owner_name = str(soup.findAll('td',attrs = {'align':'CENTER'})[2]).split('>')[1].split('<')[0].strip()\n",
        "            except:\n",
        "                owner_name = soup.findAll('td',attrs = {'align':'CENTER'})[2].text.strip()\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'lee': \n",
        "        owner_find_url = f'https://www.leepa.org/Scripts/PropertyQuery/PropertyQuery.aspx?STRAP={parcel_id}'\n",
        "        driver.get(owner_find_url)\n",
        "        try:\n",
        "            driver.find_element_by_id(\"btnContinue\").click()\n",
        "            sleep(1)\n",
        "        except:\n",
        "            pass\n",
        "        element = WebDriverWait(driver, 10).until(\n",
        "                            EC.presence_of_element_located((By.CLASS_NAME, \"textPanel\"))\n",
        "                        )\n",
        "        soup  = bs(driver.page_source,'html.parser')\n",
        "        try:\n",
        "            own = soup.findAll('div',attrs  ={'class':'textPanel'})[0].find('div')\n",
        "            lines = str(own).split('</div>')[0].split('<div>')[1].strip().split('<br/>')\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            line_one = lines[0]\n",
        "        except:\n",
        "            line_one = ''\n",
        "        try:\n",
        "            line_two = lines[1]\n",
        "        except:\n",
        "            line_two = ''\n",
        "        try:\n",
        "            line_three = lines[2]\n",
        "        except:\n",
        "            line_three = ''\n",
        "\n",
        "        owner_name = ''\n",
        "        if line_one[0].isnumeric():\n",
        "            owner_name = owner_name + line_one\n",
        "            if line_one[-1] == '+':\n",
        "                owner_name = owner_name +' '+  line_two\n",
        "            if line_one[-2:] == 'OF':\n",
        "                owner_name = owner_name +' '+  line_two\n",
        "            if line_two[-1] == '+':\n",
        "                owner_name = owner_name + ' ' + line_three\n",
        "        else:\n",
        "            owner_name = owner_name + line_one\n",
        "            if line_one[-1] == '+':\n",
        "                owner_name = owner_name +' '+  line_two\n",
        "            if line_one[-2:] == 'OF':\n",
        "                owner_name = owner_name +' '+  line_two\n",
        "            if line_two[-1] == '+':\n",
        "                owner_name = owner_name + ' ' + line_three\n",
        "        owner_name = owner_name.replace('&amp;','&').strip()\n",
        "\n",
        "    elif county == 'polk': \n",
        "        owner_find_url = f'https://www.polkpa.org/CamaDisplay.aspx?OutputMode=Display&SearchType=RealEstate&Page=FindByID&ParcelID={new_parcel_id}&cookie_test=true'\n",
        "        r = requests.get(owner_find_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "        try:\n",
        "            try:\n",
        "                owner_name = soup.find('tr',attrs= {'class':'tr1'}).find('td').text\n",
        "            except:\n",
        "                owner_name = ''\n",
        "            if 'Address 1'==owner_name:\n",
        "                owner_name = ''\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'manatee': \n",
        "        owner_find_url = f'https://www.polkpa.org/CamaDisplay.aspx?OutputMode=Display&SearchType=RealEstate&Page=FindByID&ParcelID={new_parcel_id}&cookie_test=true'\n",
        "        r = requests.get(owner_find_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "        try:\n",
        "            try:\n",
        "                owner_name = soup.find('tr',attrs= {'class':'tr1'}).find('td').text\n",
        "            except:\n",
        "                owner_name = ''\n",
        "            if 'Address 1'==owner_name:\n",
        "                owner_name = ''\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'stlucie': \n",
        "        url = f'https://paslc.gov/recordcarddata/api/re/PropertyIndex/{parcel_id}'\n",
        "        try:\n",
        "            r = requests.get(url,headers =headers)\n",
        "            soup = bs(r.content,'html.parser')\n",
        "            acc_num = json.loads(str(soup)).get('AccountNumber')\n",
        "            url = f'https://paslc.gov/recordcarddata/api/re/baserecordcard/{acc_num}'\n",
        "\n",
        "            r = requests.get(url,headers =headers)\n",
        "            soup = bs(r.content,'html.parser')\n",
        "            owner_name = json.loads(str(soup))[0].get('Owner1')\n",
        "        except:\n",
        "            owner_name = ''\n",
        "    elif county == 'marion': \n",
        "        url = f'http://www.pa.marion.fl.us/PRC.aspx?key={parcel_id}&YR=2022&mName=False&mSitus=False'\n",
        "        try:\n",
        "            r = requests.get(url,headers =headers)\n",
        "            soup = bs(r.content,'html.parser')\n",
        "\n",
        "            owner = soup.findAll('table',attrs = {'style':'width:100%;'})[3].find('tr').text\n",
        "            owner_name = ''\n",
        "            for own in owner:\n",
        "                if own.isnumeric():\n",
        "                    break\n",
        "                else:\n",
        "                    owner_name = owner_name + own\n",
        "            owner_name = owner_name.replace('More Names','')\n",
        "        except:\n",
        "            owner_name = ''\n",
        "\n",
        "    else:\n",
        "        owner_name = ''\n",
        "\n",
        "    datan = {\n",
        "        'Date': date,\n",
        "        'Type': types,\n",
        "        'Auction Status': auction_status,\n",
        "        'Sold Amount': sold_amount,\n",
        "        'Sold To': sold_to,\n",
        "        'Owner Name': owner_name,\n",
        "        'Auction Type': auction_type,\n",
        "        'Case #': case_hash,\n",
        "        'Final Judgement Amount': final_jud_amount,\n",
        "        'Parcel ID': parcel_id,\n",
        "        'Property Address': property_address,\n",
        "        'Assessed Value': assessed_value,\n",
        "        'Plaintiff Max Bid': plaintiff_max_bid\n",
        "    }\n",
        "    listan.append(datan)\n",
        "\n",
        "\n",
        "def scrape(month,county):\n",
        "    print(f'Scraping: {month} {county}')\n",
        "    month_url = f'https://{county}.realforeclose.com/index.cfm?zaction=user&zmethod=calendar&selCalDate=%7Bts%20%27{month}-01%2000%3A00%3A00%27%7D'\n",
        "    r = requests.get(month_url,headers = headers)\n",
        "    soup = bs(r.content,'html.parser')\n",
        "\n",
        "    available_dates = soup.findAll('div',attrs = {'role':'link'})\n",
        "    s = [str(av['dayid']).split('/')[1] for av in available_dates]\n",
        "    print(f'Available Dates: {s}\\n')\n",
        "\n",
        "    month_year = str(available_dates[0]['aria-label']).split('-',1)\n",
        "    month_year = month_year[0] +'-'+ month_year[1].split('-')[-1]\n",
        "    available_date_urls = [f'https://{county}.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE='+str(av['dayid']) for av in available_dates]\n",
        "    for url in available_date_urls:\n",
        "        print(f'Scraping URL: {url}')\n",
        "        driver.get(url)\n",
        "        element = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.ID, \"Area_C\"))\n",
        "            )\n",
        "        soup = bs(driver.page_source,'html.parser')\n",
        "        try:\n",
        "            running_aucs = soup.find('div',attrs = {'id':'Area_R'}).findAll('div',recursive=False)\n",
        "        except:\n",
        "            running_aucs = []\n",
        "        try:\n",
        "            waiting_aucs = soup.find('div',attrs = {'id':'Area_W'}).findAll('div',recursive=False)\n",
        "        except:\n",
        "            waiting_aucs = []\n",
        "        try:\n",
        "            closed_aucs = soup.find('div',attrs = {'id':'Area_C'}).findAll('div',recursive=False)\n",
        "        except:\n",
        "            closed_aucs = []\n",
        "\n",
        "        if len(closed_aucs)>0:\n",
        "            for closed in closed_aucs:\n",
        "                date = url.split('=')[-1]\n",
        "                types = 'Auctions Closed or Canceled'\n",
        "                try:\n",
        "                    left_side = closed.find('div',attrs = {'class':'AUCTION_STATS'})\n",
        "                    try:\n",
        "                        right_side = closed.find('div',attrs = {'class':'AUCTION_DETAILS'})\n",
        "                    except:\n",
        "                        right_side = closed.find('div',attrs = {'class':'AUCTION_DETAILS adc'})\n",
        "                    auction_scrape(date,types,left_side,right_side,county)\n",
        "                except:\n",
        "                    pass\n",
        "        if len(running_aucs)>0:\n",
        "            for running in running_aucs:\n",
        "                date = url.split('=')[-1]\n",
        "                types = 'Auctions Running'\n",
        "                try:\n",
        "                    left_side = running.find('div',attrs = {'class':'AUCTION_STATS'})\n",
        "                    try:\n",
        "                        right_side = running.find('div',attrs = {'class':'AUCTION_DETAILS'})\n",
        "                    except:\n",
        "                        right_side = running.find('div',attrs = {'class':'AUCTION_DETAILS adc'})\n",
        "                    auction_scrape(date,types,left_side,right_side,county)\n",
        "                except:\n",
        "                    pass\n",
        "        if len(waiting_aucs)>0:\n",
        "            for waiting in waiting_aucs:\n",
        "                date = url.split('=')[-1]\n",
        "                types = 'Auctions Waiting'\n",
        "                try:\n",
        "                    left_side = waiting.find('div',attrs = {'class':'AUCTION_STATS'})\n",
        "                    try:\n",
        "                        right_side = waiting.find('div',attrs = {'class':'AUCTION_DETAILS'})\n",
        "                    except:\n",
        "                        right_side = closed.find('div',attrs = {'class':'AUCTION_DETAILS adc'})\n",
        "                    auction_scrape(date,types,left_side,right_side,county)\n",
        "                except:\n",
        "                    pass\n",
        "    date = url.split('=')[-1].replace('/','-')\n",
        "    \n",
        "    df = pd.DataFrame(listan).to_excel(f'{county}_{month_year}.xlsx',encoding = \"utf-8-sig\",index = False)\n",
        "    from google.colab import files\n",
        "    files.download(f'{county}_{month_year}.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # list_of_county = ['duval','miamidade','broward','hillsborough','orange','pinellas','lee','polk','pasco','manatee','stlucie','marion']\n",
        "    # count = 1\n",
        "    # for county in list_of_county:\n",
        "    #     print(f'{count}. Scrape {county}')\n",
        "    #     count = count + 1\n",
        "    # choice = int(input('Enter Choice: ')) - 1\n",
        "    # print('\\n')\n",
        "    # date_choice = input('Enter Year-Month(Ex. 2022-06): ')\n",
        "    # scrape(date_choice,list_of_county[choice])\n",
        "    scrape('2022-07','lee')\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "3UU0OlIqhqM_",
        "outputId": "8244f657-94ca-4d49-9f57-ad243bb6c0cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: 2022-07 lee\n",
            "Available Dates: ['06', '08', '11', '13', '20', '21', '22', '27']\n",
            "\n",
            "Scraping URL: https://lee.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE=07/06/2022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:189: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping URL: https://lee.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE=07/08/2022\n",
            "Scraping URL: https://lee.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE=07/11/2022\n",
            "Scraping URL: https://lee.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE=07/13/2022\n",
            "Scraping URL: https://lee.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE=07/20/2022\n",
            "Scraping URL: https://lee.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE=07/21/2022\n",
            "Scraping URL: https://lee.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE=07/22/2022\n",
            "Scraping URL: https://lee.realforeclose.com/index.cfm?zaction=AUCTION&Zmethod=PREVIEW&AUCTIONDATE=07/27/2022\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dda176b3-4815-4252-922c-23965934f86f\", \"lee_July-2022.xlsx\", 7291)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "owner_find_url = f'https://www.leepa.org/Scripts/PropertyQuery/PropertyQuery.aspx?STRAP=04-48-25-B4-01200.0410'\n",
        "driver.get(owner_find_url)\n",
        "try:\n",
        "    driver.find_element_by_id(\"btnContinue\").click()\n",
        "    sleep(1)\n",
        "except:\n",
        "    pass\n",
        "element = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CLASS_NAME, \"textPanel\"))\n",
        "                )\n",
        "soup  = bs(driver.page_source,'html.parser')\n",
        "try:\n",
        "    own = soup.findAll('div',attrs  ={'class':'textPanel'})[0].find('div')\n",
        "    lines = str(own).split('</div>')[0].split('<div>')[1].strip().split('<br/>')\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    line_one = lines[0]\n",
        "except:\n",
        "    line_one = ''\n",
        "try:\n",
        "    line_two = lines[1]\n",
        "except:\n",
        "    line_two = ''\n",
        "try:\n",
        "    line_three = lines[2]\n",
        "except:\n",
        "    line_three = ''\n",
        "\n",
        "owner_name = ''\n",
        "if line_one[0].isnumeric():\n",
        "    owner_name = owner_name + line_one\n",
        "    if line_one[-1] == '+':\n",
        "        owner_name = owner_name +' '+  line_two\n",
        "    if line_two[-1] == '+':\n",
        "        owner_name = owner_name + ' ' + line_three\n",
        "else:\n",
        "    owner_name = owner_name + line_one\n",
        "    if line_one[-1] == '+':\n",
        "        owner_name = owner_name +' '+  line_two\n",
        "    if line_two[-1] == '+':\n",
        "        owner_name = owner_name + ' ' + line_three\n",
        "owner_name = owner_name.strip()"
      ],
      "metadata": {
        "id": "8BAyFX4KtLMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}